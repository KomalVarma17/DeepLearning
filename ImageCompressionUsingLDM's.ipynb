{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Encoder"
      ],
      "metadata": {
        "id": "KXrXZdrCjnrr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HzyE5rPjgYn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Residual function block\n",
        "def resnet_block(x, channels):\n",
        "    shortcut = x\n",
        "    x = F.conv2d(x, weight1, bias=bias1, stride=1, padding=1)\n",
        "    x = F.batch_norm(x, running_mean1, running_var1, weight_bn1, bias_bn1, training=True)\n",
        "    x = F.relu(x)\n",
        "    x = F.conv2d(x, weight2, bias=bias2, stride=1, padding=1)\n",
        "    x = F.batch_norm(x, running_mean2, running_var2, weight_bn2, bias_bn2, training=True)\n",
        "    return F.relu(x + shortcut)\n",
        "\n",
        "\n",
        "def build_encoder():\n",
        "    layers = []\n",
        "\n",
        "    # First downsampling\n",
        "    layers.append(nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3))\n",
        "    layers.append(nn.BatchNorm2d(64))\n",
        "    layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "    # Residual block 1\n",
        "    layers.append(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1))\n",
        "    layers.append(nn.BatchNorm2d(64))\n",
        "    layers.append(nn.ReLU(inplace=True))\n",
        "    layers.append(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1))\n",
        "    layers.append(nn.BatchNorm2d(64))\n",
        "    layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "    # Second downsampling\n",
        "    layers.append(nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1))\n",
        "    layers.append(nn.BatchNorm2d(128))\n",
        "    layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "    # Residual block 2\n",
        "    layers.append(nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1))\n",
        "    layers.append(nn.BatchNorm2d(128))\n",
        "    layers.append(nn.ReLU(inplace=True))\n",
        "    layers.append(nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1))\n",
        "    layers.append(nn.BatchNorm2d(128))\n",
        "    layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "    # Third downsampling\n",
        "    layers.append(nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1))\n",
        "    layers.append(nn.BatchNorm2d(128))\n",
        "    layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "    # Residual block 3\n",
        "    layers.append(nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1))\n",
        "    layers.append(nn.BatchNorm2d(128))\n",
        "    layers.append(nn.ReLU(inplace=True))\n",
        "    layers.append(nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1))\n",
        "    layers.append(nn.BatchNorm2d(128))\n",
        "    layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "encoder = build_encoder()\n",
        "x = torch.randn(2, 3, 256, 256)\n",
        "z = encoder(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_forward(x, encoder):\n",
        "    skips = []\n",
        "    idx = 0\n",
        "\n",
        "\n",
        "    # First Downsampling\n",
        "    x = encoder[idx](x); idx += 1\n",
        "    x = encoder[idx](x); idx += 1\n",
        "    x = encoder[idx](x); idx += 1\n",
        "\n",
        "\n",
        "    x = encoder[idx](x); idx += 1\n",
        "    x = encoder[idx](x); idx += 1\n",
        "    x = encoder[idx](x); idx += 1\n",
        "    x = encoder[idx](x); idx += 1\n",
        "    skips.append(x)  # Save after first Residual block\n",
        "\n",
        "    # Second Downsampling\n",
        "    x = encoder[idx](x); idx += 1\n",
        "    x = encoder[idx](x); idx += 1\n",
        "    x = encoder[idx](x); idx += 1\n",
        "\n",
        "\n",
        "    x = encoder[idx](x); idx += 1\n",
        "    x = encoder[idx](x); idx += 1\n",
        "    x = encoder[idx](x); idx += 1\n",
        "    x = encoder[idx](x); idx += 1\n",
        "    skips.append(x)  # Save after second Residual block\n",
        "\n",
        "    # Third Downsampling\n",
        "    x = encoder[idx](x); idx += 1\n",
        "    x = encoder[idx](x); idx += 1\n",
        "    x = encoder[idx](x); idx += 1\n",
        "\n",
        "\n",
        "    x = encoder[idx](x); idx += 1\n",
        "    x = encoder[idx](x); idx += 1\n",
        "    x = encoder[idx](x); idx += 1\n",
        "    x = encoder[idx](x); idx += 1\n",
        "    skips.append(x)  # Save after third Residual block\n",
        "\n",
        "    return x, skips\n"
      ],
      "metadata": {
        "id": "ZwIup60LjraL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder"
      ],
      "metadata": {
        "id": "g_jUpJQCju_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsampling Block (upsample spatial size and reduce channels)\n",
        "def up_block(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "# U-Net Decoder\n",
        "def build_unet(latent_channels=128, base_channels=64):\n",
        "    encoder_channels = [64, 128, 128]  # Channels from the encoder at different levels\n",
        "\n",
        "    decoder_layers = nn.ModuleList([\n",
        "        up_block(latent_channels, encoder_channels[-1]),               # 128 → 128 (for skip3)\n",
        "        up_block(encoder_channels[-1]*2, encoder_channels[-2]),         # 128+128 → 128 (for skip2)\n",
        "        up_block(encoder_channels[-2]*2, encoder_channels[-3]),\n",
        "        nn.Conv2d(encoder_channels[-3]*2, 3, kernel_size=3, stride=1, padding=1)  # 64+64 → 3 (final output)\n",
        "    ])\n",
        "\n",
        "    return decoder_layers\n",
        "\n",
        "# Forward Function for Decoder\n",
        "def forward_unet(x, skips, decoder_layers):\n",
        "    for idx, layer in enumerate(decoder_layers[:-1]):  # Except the final conv\n",
        "        x = layer(x)\n",
        "\n",
        "        if idx < len(skips):\n",
        "            skip = skips[-(idx + 1)]  # Take deepest skip first\n",
        "            skip = F.interpolate(skip, size=x.shape[-2:], mode='nearest')  # Match spatial size\n",
        "            x = torch.cat([x, skip], dim=1)  # Concatenate along channels\n",
        "\n",
        "    # Final convolution to 3-channel output\n",
        "    x = decoder_layers[-1](x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "haiUlIbOjtkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diffusion"
      ],
      "metadata": {
        "id": "FjeoxhDRj0QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Beta schedule\n",
        "def get_beta_schedule(n_timesteps, beta_start=1e-4, beta_end=0.02):\n",
        "    return torch.linspace(beta_start, beta_end, n_timesteps)\n",
        "\n",
        "# Compute alpha and cumulative alpha\n",
        "def compute_alpha_terms(betas):\n",
        "    alphas = 1.0 - betas\n",
        "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "    return alphas, alphas_cumprod\n",
        "\n",
        "# Sampling function\n",
        "def q_sample(x0, t, alphas_cumprod):\n",
        "    B, C, H, W = x0.shape\n",
        "    noise = torch.randn_like(x0)\n",
        "    alpha_t = alphas_cumprod[t].view(B, 1, 1, 1)\n",
        "    x_n = torch.sqrt(alpha_t) * x0 + torch.sqrt(1 - alpha_t) * noise\n",
        "    return x_n, noise\n"
      ],
      "metadata": {
        "id": "iEydyyUCjyYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function"
      ],
      "metadata": {
        "id": "JMvuk16Wj4qF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(predicted_x0, true_x0):\n",
        "    return F.mse_loss(predicted_x0, true_x0)\n"
      ],
      "metadata": {
        "id": "ywG8S3xKj19a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Datasets"
      ],
      "metadata": {
        "id": "VSwptN63j8jU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "# 1. Get all image file paths from a directory\n",
        "def get_image_paths(directory):\n",
        "    image_paths = []\n",
        "    for file in os.listdir(directory):\n",
        "        if file.endswith('.png'):\n",
        "            full_path = os.path.join(directory, file)\n",
        "            image_paths.append(full_path)\n",
        "    image_paths = sorted(image_paths)\n",
        "    return image_paths\n",
        "\n",
        "# 2. Load and process one image\n",
        "def process_image(image_path, crop_size=256):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    w, h = image.size\n",
        "\n",
        "    # Crop if large enough, else resize\n",
        "    if w >= crop_size and h >= crop_size:\n",
        "        top = random.randint(0, h - crop_size)\n",
        "        left = random.randint(0, w - crop_size)\n",
        "        image = TF.crop(image, top, left, crop_size, crop_size)\n",
        "    else:\n",
        "        image = TF.resize(image, (crop_size, crop_size))\n",
        "\n",
        "    return TF.to_tensor(image)\n",
        "\n",
        "# 3. Create a batch (Single simplified line)\n",
        "def create_batch(image_paths, batch_size=4, crop_size=256):\n",
        "    selected_paths = random.sample(image_paths, batch_size)\n",
        "    batch = [process_image(p, crop_size) for p in selected_paths]\n",
        "    return torch.stack(batch)\n"
      ],
      "metadata": {
        "id": "MGvzadavj6Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your dataset paths\n",
        "train_dir = '/content/drive/MyDrive/datasets/DIV2K_train_HR'\n",
        "val_dir = '/content/drive/MyDrive/datasets/DIV2K_valid_HR'\n",
        "\n",
        "# Get image file paths\n",
        "train_image_paths = get_image_paths(train_dir)\n",
        "val_image_paths = get_image_paths(val_dir)\n",
        "\n",
        "# Create a batch\n",
        "train_batch = create_batch(train_image_paths, batch_size=4, crop_size=256)\n",
        "\n",
        "print(f\"Train batch shape: {train_batch.shape}\")"
      ],
      "metadata": {
        "id": "h0A64mS7kASo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "VvmJRNzskD3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import os\n",
        "\n",
        "# 1. Initialize model parts\n",
        "encoder = build_encoder()\n",
        "decoder_layers = build_unet(latent_channels=128, base_channels=64)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "encoder = encoder.to(device)\n",
        "decoder_layers = decoder_layers.to(device)\n",
        "\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder_layers.parameters()), lr=5e-4)\n",
        "\n",
        "n_timesteps = 1000\n",
        "betas = get_beta_schedule(n_timesteps)\n",
        "alphas, alphas_cumprod = compute_alpha_terms(betas)\n",
        "alphas_cumprod = alphas_cumprod.to(device)\n",
        "\n",
        "train_image_paths = get_image_paths('/dgxa_home/se22uari031/DIV2K_train_HR')\n",
        "val_image_paths = get_image_paths('/dgxa_home/se22uari031/DIV2K_valid_HR')\n",
        "checkpoint_dir = '/dgxa_home/se22uari031/model_checkpoints'\n",
        "best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
        "\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "def compute_psnr(img1, img2):\n",
        "    mse = F.mse_loss(img1, img2)\n",
        "    if mse == 0: return 100\n",
        "    return 20 * torch.log10(1.0 / torch.sqrt(mse)).item()\n",
        "\n",
        "def validate_model(encoder, decoder_layers, val_image_paths, batch_size=4, crop_size=256):\n",
        "    encoder.eval()\n",
        "    decoder_layers.eval()\n",
        "    total_loss = 0.0\n",
        "    total_psnr = 0.0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(len(val_image_paths) // batch_size):\n",
        "            real_images = create_batch(val_image_paths, batch_size=batch_size, crop_size=crop_size).to(device)\n",
        "            latent, skips = encoder_forward(real_images, encoder)\n",
        "            predicted_x0 = forward_unet(latent, skips, decoder_layers)\n",
        "            loss = loss_function(predicted_x0, real_images)\n",
        "            psnr_val = compute_psnr(predicted_x0, real_images)\n",
        "            total_loss += loss.item()\n",
        "            total_psnr += psnr_val\n",
        "            count += 1\n",
        "\n",
        "    avg_loss = total_loss / count\n",
        "    avg_psnr = total_psnr / count\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"[Validation] Loss: {avg_loss:.6f}, PSNR: {avg_psnr:.2f} dB\")\n",
        "    encoder.train()\n",
        "    decoder_layers.train()\n",
        "    return avg_loss, avg_psnr\n",
        "\n",
        "\n",
        "num_epochs = 300\n",
        "batch_size = 32\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_val_psnr = 0.0\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for _ in range(len(train_image_paths) // batch_size):\n",
        "        real_images = create_batch(train_image_paths, batch_size=batch_size, crop_size=256).to(device)\n",
        "        latent, skips = encoder_forward(real_images, encoder)\n",
        "        t = torch.randint(0, n_timesteps, (real_images.shape[0],)).to(device)\n",
        "        noisy_latent, noise = q_sample(latent, t, alphas_cumprod)\n",
        "        predicted_x0 = forward_unet(noisy_latent, skips, decoder_layers)\n",
        "        loss = loss_function(predicted_x0, real_images)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / (len(train_image_paths) // batch_size)\n",
        "    print(f\"[Epoch {epoch+1}/{num_epochs}] Average Loss: {avg_loss:.6f}\")\n",
        "\n",
        "    # Validate every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        val_loss, val_psnr = validate_model(encoder, decoder_layers, val_image_paths, batch_size=4)\n",
        "\n",
        "        # SIMPLE direct check: save if ANY metric improves\n",
        "        if (val_loss < best_val_loss) or (val_psnr > best_val_psnr):\n",
        "            best_val_loss = min(best_val_loss, val_loss)\n",
        "            best_val_psnr = max(best_val_psnr, val_psnr)\n",
        "\n",
        "            torch.save({\n",
        "                'encoder_state_dict': encoder.state_dict(),\n",
        "                'decoder_state_dict': decoder_layers.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'metrics': {'val_loss': val_loss, 'val_psnr': val_psnr}\n",
        "            }, best_model_path)\n",
        "            print(f\"[Best Model Saved] Epoch {epoch+1}\")\n",
        "\n",
        "    # Checkpoint every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f\"model_checkpoint_epoch_{epoch+1}.pth\")\n",
        "        torch.save({\n",
        "            'encoder_state_dict': encoder.state_dict(),\n",
        "            'decoder_state_dict': decoder_layers.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'epoch': epoch\n",
        "        }, checkpoint_path)\n",
        "        print(f\"[Checkpoint Saved] at: {checkpoint_path}\")\n"
      ],
      "metadata": {
        "id": "fFQYrgzwkDJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load The Trained Model"
      ],
      "metadata": {
        "id": "2rv6PniulQD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 1. Rebuild the encoder and decoder (same architecture as during training)\n",
        "encoder = build_encoder()\n",
        "decoder_layers = build_unet(latent_channels=128, base_channels=64)\n",
        "\n",
        "# 2. Move models to device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "encoder = encoder.to(device)\n",
        "decoder_layers = decoder_layers.to(device)\n",
        "\n",
        "# 3. Load saved checkpoint\n",
        "checkpoint_path = '/dgxa_home/se22uari031/model_checkpoints/best_model.pth'\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "# 4. Load the model parameters\n",
        "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "decoder_layers.load_state_dict(checkpoint['decoder_state_dict'])\n",
        "\n",
        "# 5. Set models to eval\n",
        "encoder.eval()\n",
        "decoder_layers.eval()"
      ],
      "metadata": {
        "id": "D3iMGC95krgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation on Loss\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T7a-CeNSlZEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# Function to compute PSNR\n",
        "def compute_psnr(img1, img2):\n",
        "    mse = F.mse_loss(img1, img2)\n",
        "    if mse == 0:\n",
        "        return 100\n",
        "    psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
        "    return psnr.item()\n",
        "\n",
        "# Validation Function\n",
        "def validate_model(encoder, decoder_layers, val_image_paths, batch_size=4, crop_size=256):\n",
        "    encoder.eval()\n",
        "    decoder_layers.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_psnr = 0.0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(len(val_image_paths) // batch_size):\n",
        "            real_images = create_batch(val_image_paths, batch_size=batch_size, crop_size=crop_size)\n",
        "            real_images = real_images.to(device)\n",
        "\n",
        "            latent, skips = encoder_forward(real_images, encoder)\n",
        "            predicted_x0 = forward_unet(latent, skips, decoder_layers)\n",
        "\n",
        "            loss = loss_function(predicted_x0, real_images)\n",
        "            psnr = compute_psnr(predicted_x0, real_images)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_psnr += psnr\n",
        "            count += 1\n",
        "\n",
        "    avg_loss = total_loss / count\n",
        "    avg_psnr = total_psnr / count\n",
        "\n",
        "    print(f\"Validation - Average Loss: {avg_loss:.6f}, Average PSNR: {avg_psnr:.2f} dB\")\n",
        "\n",
        "    encoder.train()\n",
        "    decoder_layers.train()\n"
      ],
      "metadata": {
        "id": "Zog8DWyflR02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_image_paths = get_image_paths('/content/drive/MyDrive/datasets/DIV2K_valid_HR')\n",
        "\n",
        "validate_model(encoder, decoder_layers, val_image_paths, batch_size=4, crop_size=256)\n"
      ],
      "metadata": {
        "id": "KBNBUcozlfCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display Images"
      ],
      "metadata": {
        "id": "bsUH3jjRlwB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# Function to generate reconstructed images from real images\n",
        "def reconstruct_images(encoder, decoder_layers, real_images):\n",
        "    encoder.eval()\n",
        "    decoder_layers.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        latent, skips = encoder_forward(real_images, encoder)\n",
        "        predicted_x0 = forward_unet(latent, skips, decoder_layers)\n",
        "\n",
        "    return predicted_x0\n",
        "\n",
        "# Function to display input vs output side-by-side\n",
        "def display_images(real_images, reconstructed_images, idx=0):\n",
        "    \"\"\"\n",
        "    real_images: batch of real images [B, 3, H, W]\n",
        "    reconstructed_images: batch of predicted images [B, 3, H, W]\n",
        "    idx: which image in the batch to display\n",
        "    \"\"\"\n",
        "\n",
        "    # Move to CPU and prepare for display\n",
        "    real_img = real_images[idx].detach().cpu().numpy()\n",
        "    real_img = real_img.transpose(1, 2, 0)  # C, H, W -> H, W, C\n",
        "    real_img = real_img.clip(0, 1)\n",
        "\n",
        "    recon_img = reconstructed_images[idx].detach().cpu().numpy()\n",
        "    recon_img = recon_img.transpose(1, 2, 0)\n",
        "    recon_img = recon_img.clip(0, 1)\n",
        "\n",
        "    # Plot\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    axs[0].imshow(real_img)\n",
        "    axs[0].set_title(\"Original Image\")\n",
        "    axs[0].axis('off')\n",
        "\n",
        "    axs[1].imshow(recon_img)\n",
        "    axs[1].set_title(\"Reconstructed Image\")\n",
        "    axs[1].axis('off')\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "jxMaxLxSlvxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "val_image_paths = get_image_paths('/content/drive/MyDrive/datasets/DIV2K_valid_HR')\n",
        "\n",
        "# Create a small batch\n",
        "real_images = create_batch(val_image_paths, batch_size=4, crop_size=256)\n",
        "real_images = real_images.to(device)\n",
        "\n",
        "# Generate reconstructed images\n",
        "reconstructed_images = reconstruct_images(encoder, decoder_layers, real_images)\n",
        "\n",
        "\n",
        "display_images(real_images, reconstructed_images, idx=0)\n",
        "display_images(real_images, reconstructed_images, idx=1)\n",
        "display_images(real_images, reconstructed_images, idx=2)"
      ],
      "metadata": {
        "id": "XIidVc2Qly3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Compression Ratio"
      ],
      "metadata": {
        "id": "BQ-sAqx7l3eO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pixel based\n",
        "input_image_size = real_images.shape[1] * real_images.shape[2] * real_images.shape[3]\n",
        "latent_size = latent.shape[1] * latent.shape[2] * latent.shape[3]\n",
        "\n",
        "print(f\"Input image values{input_image_size}\")\n",
        "print(f\"Latent values: {latent_size}\")\n",
        "compression_ratio = input_image_size / latent_size\n",
        "print(f\"Compression Ratio: {compression_ratio:.2f}x\")\n"
      ],
      "metadata": {
        "id": "9diIPauol3Np"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#memory based\n",
        "def tensor_size_bytes(tensor):\n",
        "    return tensor.nelement() * tensor.element_size()\n",
        "\n",
        "input_size = tensor_size_bytes(real_images)\n",
        "latent_size = tensor_size_bytes(latent)\n",
        "\n",
        "compression_ratio = input_size / latent_size\n",
        "\n",
        "print(f\"Input image size: {input_size / 1e6:.4f} MB\")\n",
        "print(f\"Latent tensor size: {latent_size / 1e6:.4f} MB\")\n",
        "print(f\"Compression ratio: {compression_ratio:.2f}x\")\n"
      ],
      "metadata": {
        "id": "FIDO-eITpRSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test on an Image"
      ],
      "metadata": {
        "id": "BqAmD0I7sms3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "encoder = build_encoder()\n",
        "decoder_layers = build_unet(latent_channels=128, base_channels=64)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "encoder = encoder.to(device)\n",
        "decoder_layers = decoder_layers.to(device)\n",
        "\n",
        "checkpoint = torch.load('/content/drive/MyDrive/datasets/best_model.pth', map_location=device)\n",
        "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "decoder_layers.load_state_dict(checkpoint['decoder_state_dict'])\n",
        "\n",
        "encoder.eval()\n",
        "decoder_layers.eval()\n",
        "\n",
        "\n",
        "test_image_path = '/content/sample_photos/FjikPptEbZg.jpg'\n",
        "\n",
        "def process_image(image_path, crop_size=256):\n",
        "    from PIL import Image\n",
        "    import torchvision.transforms as T\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    transform = T.Compose([\n",
        "        T.Resize((crop_size, crop_size)),\n",
        "        T.ToTensor()\n",
        "    ])\n",
        "    return transform(image)\n",
        "\n",
        "real_image = process_image(test_image_path).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "latent1, skips = encoder_forward(real_image, encoder)\n",
        "\n",
        "\n",
        "reconstructed_image = forward_unet(latent1, skips, decoder_layers)\n",
        "\n",
        "\n",
        "reconstructed_image = torch.clamp(reconstructed_image, 0, 1)\n",
        "\n",
        "\n",
        "def compute_psnr(img1, img2):\n",
        "    import torch.nn.functional as F\n",
        "    mse = F.mse_loss(img1, img2)\n",
        "    return 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
        "\n",
        "psnr_val = compute_psnr(reconstructed_image, real_image)\n",
        "print(f\"PSNR on {test_image_path}: {psnr_val:.2f} dB\")\n",
        "\n",
        "\n",
        "save_image(reconstructed_image, f'reconstructed_{test_image_path.split(\"/\")[-1]}')\n"
      ],
      "metadata": {
        "id": "7sfPs5oepM23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the image"
      ],
      "metadata": {
        "id": "ptnc9hRVstNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Move tensors to CPU and convert to numpy for visualization\n",
        "real_image_np = real_image.squeeze(0).permute(1, 2, 0).cpu().detach().numpy()\n",
        "reconstructed_np = reconstructed_image.squeeze(0).permute(1, 2, 0).cpu().detach().numpy()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(real_image_np)\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(reconstructed_np)\n",
        "plt.title('Reconstructed Image')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MaO26CFesuWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Images from a folder"
      ],
      "metadata": {
        "id": "H1ZXLPeewQ0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),  # Normalized to [0, 1]\n",
        "])\n",
        "\n",
        "def load_test_images(folder_path):\n",
        "    image_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
        "    images = []\n",
        "\n",
        "    for img_path in image_paths[:5]:  # Limit to first 5 images\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img = transform(img)\n",
        "        images.append(img)\n",
        "\n",
        "    return torch.stack(images), image_paths[:5]\n",
        "\n",
        "# Example: load from './test_data'\n",
        "test_images, image_paths = load_test_images('/content/sample_photos')\n",
        "print(f\"Loaded {len(test_images)} test images\")\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "encoder = encoder.to(device).eval()\n",
        "decoder_layers = decoder_layers.to(device).eval()\n",
        "\n",
        "test_images = test_images.to(device)\n",
        "\n",
        "\n",
        "latent, skips = encoder_forward(test_images, encoder)\n",
        "reconstructed_images = forward_unet(latent, skips, decoder_layers)\n",
        "\n"
      ],
      "metadata": {
        "id": "ClnCveB0wQnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SSIM and MS SSIM"
      ],
      "metadata": {
        "id": "46LYPRNWs7Vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_ssim(img1, img2):\n",
        "    img1_np = img1.permute(1, 2, 0).detach().cpu().numpy()\n",
        "    img2_np = img2.permute(1, 2, 0).detach().cpu().numpy()\n",
        "\n",
        "    score = ssim(img1_np, img2_np, channel_axis=-1, data_range=1.0)\n",
        "    return score\n",
        "\n",
        "\n",
        "for i in range(test_images.shape[0]):\n",
        "    ssim_val = compute_ssim(test_images[i], reconstructed_images[i])\n",
        "    msssim_val = ms_ssim(test_images[i].unsqueeze(0), reconstructed_images[i].unsqueeze(0), data_range=1.0).item()\n",
        "    print(f\"Image: {image_paths[i]} | SSIM: {ssim_val:.4f} | MS-SSIM: {msssim_val:.4f}\")"
      ],
      "metadata": {
        "id": "jQEc0gLXs7GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load images from folder and calculate metrics on sample images"
      ],
      "metadata": {
        "id": "6eRQ_2Igybsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Resize reconstructed images to match test_images shape\n",
        "reconstructed_images_resized = F.interpolate(reconstructed_images, size=test_images.shape[-2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "# Ensure we loop only over valid image pairs\n",
        "num_images = min(test_images.shape[0], reconstructed_images_resized.shape[0])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 4 * num_images))\n",
        "\n",
        "for i in range(num_images):\n",
        "    # Original image\n",
        "    orig_img = test_images[i].permute(1, 2, 0).detach().cpu().numpy()\n",
        "    # Resized Reconstructed image\n",
        "    recon_img = reconstructed_images_resized[i].permute(1, 2, 0).detach().cpu().numpy()\n",
        "\n",
        "    # Clamp to valid range [0, 1]\n",
        "    orig_img = orig_img.clip(0, 1)\n",
        "    recon_img = recon_img.clip(0, 1)\n",
        "\n",
        "    # Compute SSIM & MS-SSIM\n",
        "    ssim_val = compute_ssim(test_images[i], reconstructed_images_resized[i])\n",
        "    msssim_val = ms_ssim(test_images[i].unsqueeze(0), reconstructed_images_resized[i].unsqueeze(0), data_range=1.0).item()\n",
        "\n",
        "    # Plot Original\n",
        "    plt.subplot(num_images, 2, i * 2 + 1)\n",
        "    plt.imshow(orig_img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Original: {os.path.basename(image_paths[i])}\", fontsize=10)\n",
        "\n",
        "    # Plot Reconstructed\n",
        "    plt.subplot(num_images, 2, i * 2 + 2)\n",
        "    plt.imshow(recon_img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Reconstructed\\nSSIM: {ssim_val:.4f} | MS-SSIM: {msssim_val:.4f}\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NvF9TcfvsqUG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}